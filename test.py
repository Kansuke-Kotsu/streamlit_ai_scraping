import os
import streamlit as st
import google.generativeai as genai
import random
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import urllib.parse
import tiktoken

def get_information(html):
    # API Keyã‚’ã‚»ãƒƒãƒˆ
    genai.configure(api_key=st.secrets["gemini_key"])
    trimmed_html = trim_html_content(html_content=html)
    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
    prompt = f"""ã“ã‚Œã‚‰ã¯ã‚ã‚‹ECã‚µã‚¤ãƒˆã®HTMLæ–‡ã§ã™ã€‚
                {trimmed_html}
                HTMLã®å†…å®¹ã‹ã‚‰ã€ã“ã‚ŒãŒã©ã®ã‚ˆã†ãªwebã‚µã‚¤ãƒˆã§ã€ã©ã®ã‚ˆã†ãªæƒ…å ±ãŒè¼‰ã£ã¦ã„ã‚‹ã‹è§£æã—ã¦ãã ã•ã„ã€‚
                å›ç­”ã¯ã€ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ä¸€è¨€ã§æ•™ãˆã¦ãã ã•ã„ã€‚
                ä¾‹ï¼‘ï¼šã€Œã“ã‚Œã¯æ¥½å¤©å¸‚å ´ã®ä¼šå“¡ç™»éŒ²ãƒšãƒ¼ã‚¸ã§ã™ã€‚ã€
                ä¾‹ï¼’ï¼šã€Œã“ã‚Œã¯Amazonã®ã‚¤ãƒ³ãƒ†ãƒªã‚¢ã®å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã§ã™ã€‚ãƒšãƒ¼ã‚¸å†…ã«ã¯ã„ãã¤ã‹ã®å•†å“æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ã€‚
                ã€€ã€€ã€€ã€€ä¾‹ï¼‰ã‚½ãƒ•ã‚¡ãƒ¼ã€€10,000å††ãªã©ã€
                ãƒ»ãƒ»ãƒ»
                """
    # ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
    model = genai.GenerativeModel("gemini-1.5-flash")
    response = model.generate_content(prompt)
    
    # çµæœã‚’è¡¨ç¤º
    return response.text

# URLå†…ã®å…¨aã‚¿ã‚°æƒ…å ±ã‚’å–å¾—
def get_links(url):
    # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹ã‚’ç¢ºèª
        soup = BeautifulSoup(response.text, 'html.parser')
        # ã‚¢ã‚¯ã‚»ã‚¹å…ƒã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—
        base_domain = urlparse(url).netloc
        # ãƒšãƒ¼ã‚¸å†…ã®ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—
        links = soup.find_all('a', href=True)
        # ã™ã¹ã¦ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã€å¤–éƒ¨ãƒªãƒ³ã‚¯ã¯é™¤å¤–ã—ã¦ä¿å­˜
        link_data = []
        for link in links:
            full_url = urljoin(url, link['href'])  # ç›¸å¯¾ãƒªãƒ³ã‚¯ã‚’çµ¶å¯¾URLã«å¤‰æ›
            full_url = normalize_url(full_url)  # URLã‚’æ­£è¦åŒ–
            link_domain = urlparse(full_url).netloc  # ãƒªãƒ³ã‚¯ã®ãƒ‰ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã‚’å–å¾—
            # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªãƒ³ã‚¯ã®ã¿ä¿å­˜
            if link_domain == base_domain:
                link_data.append(full_url)  # URLã®ã¿ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        return link_data
    
    except requests.exceptions.RequestException as e:
        return []

# URLã‚’æ­£è¦åŒ–
def normalize_url(url):
    # URLã‚’ãƒ‘ãƒ¼ã‚¹
    parsed_url = urllib.parse.urlparse(url)
    # ã‚¹ã‚­ãƒ¼ãƒ ï¼ˆhttp or httpsï¼‰ã‚’å°æ–‡å­—ã«å¤‰æ›
    scheme = parsed_url.scheme.lower()
    # ãƒãƒƒãƒˆãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆãƒ›ã‚¹ãƒˆéƒ¨åˆ†ï¼‰ã‚’å°æ–‡å­—ã«å¤‰æ›
    netloc = parsed_url.netloc.lower()
    # æ¨™æº–ãƒãƒ¼ãƒˆã‚’å‰Šé™¤ (httpãªã‚‰80, httpsãªã‚‰443)
    if scheme == 'http' and netloc.endswith(':80'):
        netloc = netloc[:-3]  # ':80' ã‚’å‰Šé™¤
    elif scheme == 'https' and netloc.endswith(':443'):
        netloc = netloc[:-4]  # ':443' ã‚’å‰Šé™¤
    
    # ãƒ‘ã‚¹ã®æœ«å°¾ã®ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ã‚’çµ±ä¸€
    path = parsed_url.path
    #if not path.endswith('/'):
    #    path += '/'
    # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒˆã¯ã‚½ãƒ¼ãƒˆã—ã¦çµ±ä¸€
    query = urllib.parse.urlencode(sorted(urllib.parse.parse_qsl(parsed_url.query)))
    # URLã‚’å†æ§‹æˆ
    normalized_url = urllib.parse.urlunparse((scheme, netloc, path, parsed_url.params, query, ''))
    
    return normalized_url

# URLå†…ã®HTMLåˆ†ã‚’å–å¾—
def get_html(url):
    # requestsã‚’ä½¿ç”¨ã—ã¦Webãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
    headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }  
    
    response = requests.get(url, headers=headers)
    # BeautifulSoupã‚’ä½¿ã£ã¦HTMLã‚’è§£æ
    soup = BeautifulSoup(response.text, 'html.parser')
    # ãƒšãƒ¼ã‚¸ã®HTMLå…¨ä½“ã‚’å–å¾—
    html_content = soup.prettify()  # å¯èª­æ€§ã‚’å‘ä¸Šã•ã›ãŸHTMLã‚’å–å¾—

    return html_content

# ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ã„ã¦HTMLã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹é–¢æ•°
def trim_html_content(html_content, max_tokens=5000):
    # OpenAIã®GPTãƒ¢ãƒ‡ãƒ«ã«å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’å–å¾—
    encoding = tiktoken.encoding_for_model("gpt-4")
    
    # HTMLã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›
    tokenized_html = encoding.encode(html_content)
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒmax_tokensã‚’è¶…ãˆã¦ã„ãªã„å ´åˆã€ãã®ã¾ã¾è¿”ã™
    if len(tokenized_html) <= max_tokens:
        return html_content
    
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒmax_tokensã‚’è¶…ãˆãŸå ´åˆã€ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹
    trimmed_tokenized_html = tokenized_html[:max_tokens]
    
    # ãƒˆãƒªãƒŸãƒ³ã‚°ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…ƒã®HTMLæ–‡å­—åˆ—ã«æˆ»ã™
    trimmed_html_content = encoding.decode(trimmed_tokenized_html)
    
    return trimmed_html_content

# Show title and description.
st.title("ğŸ’¬ AI Scraping tool")
st.write("")
st.write("LLMã‚’é€£æºã•ã›ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚")
st.write("ãƒšãƒ¼ã‚¸ã®HTMLæ§‹é€ ã«é–¢ã‚ã‚‰ãšæƒ…å ±ã‚’å–å¾—ã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚")
st.write("ãƒ‡ãƒ¢ã¨ã—ã¦ã€ã‚µã‚¤ãƒˆå†…ã®ãƒšãƒ¼ã‚¸ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«5ã¤å–å¾—ã—ã¦ã€ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚")
st.write("â€»åŒä¸€ãƒšãƒ¼ã‚¸ã«æ„å›³çš„ã«é€£ç¶šã—ãŸã‚¢ã‚¯ã‚»ã‚¹ã‚’è¡Œã†è¡Œç‚ºã¯ã”é æ…®ãã ã•ã„ã€‚")
st.write("")

url = st.text_input("URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šhttps://www.rakuten.co.jp/ã€https://www.amazon.co.jp/ï¼‰")

if st.button("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"):
    links = get_links(url=url)
    # ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ›´æ–°
    output_area = st.empty()  # ç©ºã®é ˜åŸŸã‚’ä½œæˆã—ã¦ã€ã“ã“ã«å¾ã€…ã«ãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤ºã™ã‚‹
    if len(links) == 0:
        output_area.write("æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ") 
    else:
        response = []
        try:
            links = random.sample(links, 5)
            for i in range(5):
                html_contents = get_html(links[i])
                information = get_information(html=html_contents)
                response.append([links[i], information])
                output_area.write(response)  # ç¾åœ¨ã®å†…å®¹ã‚’è¡¨ç¤º
        except:
            output_area.write("æ™‚é–“ã‚’ãŠã„ã¦ã‚‚ã†ä¸€åº¦è©¦ã—ã¦ãã ã•ã„")
